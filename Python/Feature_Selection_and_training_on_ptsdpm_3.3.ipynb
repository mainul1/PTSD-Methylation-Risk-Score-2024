{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection. This notebook was to remove 1) Non-CpG 2) Non-CpG, X and Y chromosome probes all together  and creating a balanced training and test dataset based on case/control and male/female. In this notebook we will use only covariate adjusted data.\n",
    "\n",
    "#### Note: It use Kbest approach with Elastic net (L1 and L2 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings imported from other notebook Settings.ipynb\n",
    "%run Settings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import feather\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, datetime\n",
    "from makedirectory import make_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the program is running locally or cluster\n",
    "process = os.popen('hostname') # open process\n",
    "p_loc = process.read() \n",
    "p_loc = p_loc.strip('\\n')\n",
    "\n",
    "process.close() # close\n",
    "\n",
    "if p_loc == 'WS-IDRB-404B':\n",
    "    print(\"Running locally\")\n",
    "else:\n",
    "    print(\"Running on cluster\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use only covariate adjusted data because we saw the performance was\n",
    "# better on adjusted data. \n",
    "# Now read the imputed data adjusted for covariates\n",
    "# The adjustment was done on m values and then converted back to beta values\n",
    "if p_loc == 'WS-IDRB-404B':\n",
    "    adjusted_beta = feather.read_feather(\"G:/PGC ML/Covariate Adjusted/2022-03-09_20-58-45/Imputed_Covariate_adjusted_Meth_on_mvals_wo_Neu.feather\") \n",
    "else:\n",
    "    adjusted_beta = feather.read_feather(\"/work/a/ahwani/PGCML/Covariate Adjusted/2022-03-09_20-58-45/Imputed_Covariate_adjusted_Meth_on_mvals_wo_Neu.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted data frame:\")\n",
    "print(adjusted_beta.iloc[:,0:5].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape adjusted data {}\".format(adjusted_beta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cpg names to index\n",
    "adjusted_beta = adjusted_beta.set_index(\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_beta.index.names = [\"Basename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjusted_beta.iloc[:,0:5].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape after converting sample ids to row ids {}\".format(\n",
    "    adjusted_beta.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets remove non-CpG and X/Y chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only CpGs\n",
    "adj_beta_wo_NonCpGs = adjusted_beta.loc[:, adjusted_beta.columns.str.startswith('cg')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape after removing non-CpG probes {}\".format(adj_beta_wo_NonCpGs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_CpG specific probes removed\n",
    "non_cpg_probes = adjusted_beta.shape[1] - adj_beta_wo_NonCpGs.shape[1]\n",
    "print(\"# of non-CpG probes removed: \\n\", non_cpg_probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation file\n",
    "\n",
    "if p_loc == 'WS-IDRB-404B':\n",
    "    manifest = pd.read_csv(\"G:/EWAS meta-analysis(Janelle)/data/infinium-methylationepic-v-1-0-b5-manifest-file.csv\",\n",
    "                           skiprows=7, low_memory=False)\n",
    "else:\n",
    "    manifest = pd.read_csv(\"/home/a/ahwani/PGCML/infinium-methylationepic-v-1-0-b5-manifest-file.csv\",\n",
    "                           skiprows=7, low_memory=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print a few of two columns\n",
    "print(\"Annotation file head: \\n {}\".format(manifest[['IlmnID', 'CHR']].head(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get X and Y chromosome CpGs\n",
    "x_cpgs, y_cpgs = [manifest.loc[manifest['CHR'] == cpgs,:] for cpgs in ['X', 'Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have got only X and Y chrosome cpgs\n",
    "print(\"X and Y chromosome CpGs: \\n{}\".format(\n",
    "    [x['CHR'].value_counts() for x in [x_cpgs, y_cpgs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine X and Y chromosomes\n",
    "XY_cpgs = x_cpgs['IlmnID'].tolist() + y_cpgs['IlmnID'].tolist()\n",
    "print(\"# of X and Y probes in manifest :\", len(XY_cpgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove the cpgs from dataframe\n",
    "adj_beta_wo_NonCpGXY = adj_beta_wo_NonCpGs.drop(columns=XY_cpgs, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total probes removed i.e non-CpG, X and Y probes:\\n\",\n",
    "       adjusted_beta.shape[1] - adj_beta_wo_NonCpGXY.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape after removing non-CpG, X and Y probes:\\n\",\n",
    "      adj_beta_wo_NonCpGXY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make a dict of dfs\n",
    "adj_df_ls = {'adj_beta_wo_NonCpG':adj_beta_wo_NonCpGs, \n",
    "            'adj_beta_wo_NonCpGXY': adj_beta_wo_NonCpGXY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remaining CpGs without: \\n non-CpGs probes, non-CpGs, X chr and Y chr \\n{}\".format(\n",
    "    [x.shape for x in adj_df_ls.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check if we removed X and Y chromosome cpgs\n",
    "def how_many_cpgs(dfs, cpgs):\n",
    "    \"\"\"\n",
    "    Function to check the CpGs present\n",
    "    \"\"\"\n",
    "    print(dfs.keys())\n",
    "#     print(\"Key: {} Value {}\".format({key: value for (key, value) in dfs}))\n",
    "    print([x.columns.isin(cpgs).sum() for x in dfs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_cpgs(adj_df_ls, XY_cpgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df_ls = {key:value.iloc[:, -1000:] for key, value in adj_df_ls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"adjusted_beta shape:\", adj_df_ls['adj_beta_wo_NonCpG'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if samples in both files match\n",
    "(adj_df_ls['adj_beta_wo_NonCpG'].index == adj_df_ls['adj_beta_wo_NonCpGXY'].index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if samples in both files match\n",
    "# (adj_df_ls['adjusted_beta'].index == adj_df_ls['adjusted_beta_wo_Y'].index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load phenotype data\n",
    "# 2021-07-12_10-28-46\n",
    "import pandas as pd\n",
    "if p_loc == 'WS-IDRB-404B':\n",
    "    pheno = pd.read_csv(\"G:/PGC ML/Pre_Processed Data/2021-11-15_21-41-53/DNHS_GTP_MRS_ArmyS_Prismo_Pheno.csv\")\n",
    "else:\n",
    "    pheno = pd.read_csv(\"/home/a/ahwani/PGCML/DNHS_GTP_MRS_ArmyS_Prismo_Pheno.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of males-1 and females-2:\\n {}\".format(\n",
    "    pheno[\"Gender\"].value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only those that are in both\n",
    "pheno = pheno[pheno[\"Basename\"].isin(adjusted_beta.index)]\n",
    "pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check if samples in pheno and methylation data are in order\n",
    "(pheno[\"Basename\"] == adj_df_ls['adj_beta_wo_NonCpG'].index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sort the phenotype data according to methylation data\n",
    "pheno = pheno.set_index([\"Basename\"])\n",
    "pheno = pheno.reindex(index = adjusted_beta.index)\n",
    "pheno = pheno.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the order again\n",
    "[(pheno[\"Basename\"] == adj_df_ls[key].index).all() for key in adj_df_ls.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this phenotype file race column has strings like 1,5/2,5\n",
    "# So we need to remve the substring after , otherwise an error in ML mode\n",
    "pheno['Race'] = pheno['Race'].str.split(',').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno['Race'].str.contains(',').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in adj_df_ls.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpheno type file has some columns not needed in ML\n",
    "# Lets remove them\n",
    "# Basename we will remove later, because we need it\n",
    "# pheno = pheno.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nas values in each column\n",
    "len(pheno) - pheno.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some ptsdlife variable has missing values\n",
    "# We will remove them and use that a phenotype file \n",
    "# when using ptsdlife as outcome variable\n",
    "pheno_no_na = pheno.dropna()\n",
    "pheno_ptsdlife = pheno.dropna(subset=['Age', 'Ptsdlife', 'Race'])\n",
    "pheno_ptsdpm = pheno.dropna(subset=['Age', 'Race' ,'Ptsdpm', 'Traumanum']) # Rows with Nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pheno_no_na.shape)\n",
    "print(pheno_ptsdlife.shape)\n",
    "print(pheno_ptsdpm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdlife.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdpm.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(df, cols):\n",
    "    \"\"\"\n",
    "    Function to convert float to int\n",
    "    Parameters:\n",
    "    df: dataframe\n",
    "    cols: columns names that need conversion\n",
    "    \"\"\"\n",
    "#     df[cols] = df[cols].astype(float)\n",
    "    df = df.copy()\n",
    "    df[cols] = df[cols].astype(int)\n",
    "    return(df)\n",
    "    \n",
    "ptsdpm_cols = ['Age', 'Ptsdpm', 'Traumanum']\n",
    "ptsdlife_cols = ['Age', 'Ptsdlife', 'Traumanum'] \n",
    "\n",
    "pheno_ptsdpm = convert_to_int(df = pheno_ptsdpm, cols=ptsdpm_cols)\n",
    "pheno_ptsdlife = convert_to_int(df = pheno_ptsdlife, cols=ptsdlife_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdpm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdlife.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nas values in each column for ptsdpm and ptsdlife\n",
    "[len(x) - x.count() for x in [pheno_ptsdpm, pheno_ptsdlife]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pheno_ptsdlife\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pheno_ptsdpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control and case count\n",
    "def get_count(vals):\n",
    "    c = vals.value_counts()\n",
    "    print(\"Count ptsd: \\n{}\".format(c))\n",
    "    print(\"Percentage ptsd: \\n{}\".format(c/c.sum()))\n",
    "    \n",
    "get_count(vals = pheno_ptsdpm['Ptsdpm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_count(vals = pheno_ptsdlife['Ptsdlife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any missing values \n",
    "[k.isna().sum() for k in [pheno_ptsdpm['Ptsdpm'], \n",
    "                          pheno_ptsdlife['Ptsdlife'],\n",
    "                          pheno_ptsdpm['Pts_Severity']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will fill the missing values using column mean\n",
    "# It has three cohorts (with lifetime ptsd), and we use individual cohort \n",
    "# to impute the missing values\n",
    "def impute_nas(df, study):\n",
    "    x = df.loc[df['Study'] == study,:]\n",
    "    x.fillna(x.mean(), inplace = True)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdpm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Samples in each cohort for current ptsd:\\n{}\".format(\n",
    "    pheno_ptsdpm[\"Study\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdlife.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Samples in each cohort for lifetime ptsd:\\n{}\".format(\n",
    "pheno_ptsdlife[\"Study\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to get the data for each study\n",
    "dfs = [pheno_ptsdpm, pheno_ptsdlife]\n",
    "study = ['DNHS', 'GTP', 'MRS', \"Armystarrs\", \"Prismo\"]\n",
    "ptsd = [x.loc[x['Study'] == y,:] for x in dfs for y in study]\n",
    "ptsd = [x.fillna(x.mean()) for x in ptsd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in ptsd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After imputing the missing values, lets combine each study\n",
    "# First, DNHS,GTP and MRS for ptsdpm (at 0:5 index) and then for ptsdlife(5:10)\n",
    "pheno_ptsdpm_imp = pd.concat(ptsd[0:5], axis = 0) # all have ptsdpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdpm_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_ptsdlife_imp = pd.concat(ptsd[5:10], axis = 0) # three have ptsdlife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from each study after imputation\n",
    "print(\"Samples after imputation in each cohort for current ptsd:\\n{}\".format(\n",
    "pheno_ptsdpm_imp[\"Study\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Samples after imputation in each cohort for lifetime ptsd:\\n{}\".format(\n",
    "pheno_ptsdlife_imp[\"Study\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[x.shape for x in [pheno_ptsdpm_imp, pheno_ptsdlife_imp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As case for ptsdpm, we wanted to remove the samples\n",
    "# with no current ptsd but have lifetime ptsd\n",
    "flaged_samples = pheno_ptsdpm_imp[(pheno_ptsdpm_imp[\"Ptsdpm\"] == 0) &\n",
    "                  (pheno_ptsdpm_imp[\"Ptsdlife\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flaged samples:\",flaged_samples.shape)\n",
    "print(\"Flaged samples cohort level:\\n{}\".format(\n",
    "    flaged_samples[\"Study\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove flaged samples\n",
    "pheno_ptsdpm_imp_wo_rem = pheno_ptsdpm_imp[~pheno_ptsdpm_imp[\"Basename\"].isin(flaged_samples[\"Basename\"])]\n",
    "pheno_ptsdpm_imp_wo_rem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are remitted samples\n",
    "# it should be zero\n",
    "((pheno_ptsdpm_imp_wo_rem[\"Ptsdpm\"] == 0) &\n",
    "                  (pheno_ptsdpm_imp_wo_rem[\"Ptsdlife\"] == 1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_count(vals = pheno_ptsdpm_imp_wo_rem['Ptsdpm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.isnull().sum() for x in ptsd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "[x.isnull().any().sum() for x in adj_df_ls.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check common in ptsdpm and ptsdlife\n",
    "pheno_ptsdlife_imp[\"Basename\"].isin(pheno_ptsdpm_imp_wo_rem[\"Basename\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "\n",
    "# we will prepare three datasets, with response variables\n",
    "# ptsdpm, ptslife\n",
    "# We can use the same data for PTSDpm and PTSS\n",
    "# \"ptsdpm_imp\",\n",
    "keys = [\"ptsdpm_imp_wo_rem\",\n",
    "       \"ptsdlife_imp\"]\n",
    "values = [pheno_ptsdpm_imp_wo_rem,\n",
    "         pheno_ptsdlife_imp]\n",
    "\n",
    "def merge_data(df, vals, keys):\n",
    "    comb = [pd.merge(df, x, \n",
    "              left_index=True, \n",
    "              right_on= \"Basename\",\n",
    "              how='inner') for x in values]\n",
    "    return(dict(zip(keys, comb)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj_df_ls.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape :\\n\",[(k, adj_df_ls[k].shape) for k in adj_df_ls.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine methylation data with pheno\n",
    "combined_dfs, combined_dfs_XY = [merge_data(df = adj_df_ls[key],\n",
    "                           vals=values, keys=keys) for key in adj_df_ls.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x.keys() for x in [combined_dfs, combined_dfs_XY]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in combined_dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in combined_dfs_XY.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome variables\n",
    "# When we consider ptsd, remove PTSS from the predictors\n",
    "# When using PTSS, remove PTSD\n",
    "# Labels are same in adjusted/unadjusted data\n",
    "ptsdlife_labels = np.array(combined_dfs['ptsdlife_imp']['Ptsdlife']) # without MRS\n",
    "ptsdpm_wo_rem_labels = np.array(combined_dfs[\"ptsdpm_imp_wo_rem\"][\"Ptsdpm\"])\n",
    "ptss_labels = np.array(combined_dfs[\"ptsdpm_imp_wo_rem\"]['Pts_Severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check any na in response variables\n",
    "[np.isnan(x).any() for x in [ptsdlife_labels,\n",
    "                             ptsdpm_wo_rem_labels,\n",
    "                             ptss_labels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of outcome variable\n",
    "[len(x) for x in [ptsdpm_wo_rem_labels,\n",
    "                  ptsdlife_labels,\n",
    "                  ptss_labels\n",
    "                  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear some space\n",
    "del adjusted_beta, adj_beta_wo_NonCpGs, adj_beta_wo_NonCpGXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns that we don't need\n",
    "# keep basename, to know which samples are used in training from ptsdpm\n",
    "# and use the others samples in testing for ptsdlife \n",
    "# because some samples are different in ptsdpm and ptsdlife\n",
    "rm_cols = [\"Unnamed\", \"Ptsdpm\",\n",
    "           'Ptsdlife', 'Pts_Severity',\n",
    "           'Study', \"Avoidance\", \n",
    "           \"Hyperarousal\", \"Intrusion\",\n",
    "           \"Mdd\", \"Race\"]\n",
    "\n",
    "rm_set2 = [\"Bcell\", \"Cd4T\",  \"Cd8T\", \"Mono\",\n",
    "           \"Neu\", \"Nk\", \"Smos\", \"Age\", \n",
    "           \"Comp.2\", \"Comp.3\"] # Gender is removed while training the model\n",
    "\n",
    "# we are removing these columns for now because\n",
    "# we want to have methylation score. \n",
    " # \"Childhood_Mt\", \"Traumanum\",\n",
    "\n",
    "all_cols = rm_cols + rm_set2\n",
    "\n",
    "def drop_columns(df, cols):\n",
    "    \"\"\"\n",
    "    Function to remove columns\n",
    "    Parameters:\n",
    "    df: data frame\n",
    "    cols: columns to remove\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('|'.join(cols))]\n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop the columns we want. As we are using adjusted data now,\n",
    "# we will drop all_cols\n",
    "combined_dfs1, combined_dfs2  = [{key: drop_columns(df = value, \n",
    "                                  cols= all_cols) for (key, value) in x.items()} \n",
    "                                 for x in [combined_dfs, combined_dfs_XY]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x.keys() for x in [combined_dfs1,combined_dfs2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dfs1[\"ptsdpm_imp_wo_rem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dfs2[\"ptsdpm_imp_wo_rem\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check any nas\n",
    "[x.isnull().sum().any() for x in combined_dfs1.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.isnull().sum().any() for x in combined_dfs2.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dfs1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat dictionary of the data and labels\n",
    "qcd_data = dict({'ptsdpm_wo_NonCpGs':[combined_dfs1[\"ptsdpm_imp_wo_rem\"], ptsdpm_wo_rem_labels],\n",
    "                 'ptsdlife_wo_NonCpGs':[combined_dfs1[\"ptsdlife_imp\"], ptsdlife_labels],\n",
    "                 'ptsdpm_wo_NonCpGsXY':[combined_dfs2[\"ptsdpm_imp_wo_rem\"], ptsdpm_wo_rem_labels],\n",
    "                 'ptsdlife_wo_NonCpGsXY':[combined_dfs2[\"ptsdlife_imp\"], ptsdlife_labels]               \n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qcd_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of QCd data...\\n {}\".format([(k, qcd_data[k][0].shape) for k in qcd_data.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save qcd data\n",
    "import nbimporter\n",
    "import joblib\n",
    "import os, datetime\n",
    "from makedirectory import make_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p_loc == 'WS-IDRB-404B':\n",
    "    comb_dir = make_directory(maindir=\"G:/PGC ML/Combined Data\", verbose=True)\n",
    "else:\n",
    "    comb_dir = make_directory(maindir=\"/work/a/ahwani/PGCML/Combined_data\", verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "joblib.dump(qcd_data, os.path.join(comb_dir, \"DNHS_GTP_MRS_ArmyS_Prismo_combined.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QCd data keys:\\n{}\".format(qcd_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qcd_data[\"ptsdpm_wo_NonCpGs\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps:\n",
    "# 1. Define covariates\n",
    "# 2. Split the data into train and test\n",
    "# 3. From the train split, keep only covariate columns \n",
    "# 4. Train model using covariate columns and check performance\n",
    "# 5. Now replace the outcome variable with the predictions \n",
    "#    (expected with observed)\n",
    "# 6. Drop the covariates from train split, and using the new outcome variable\n",
    "#    train the models and check the performance \n",
    "#    (both, all features and significant)\n",
    "# 7. Adjust the models if needed\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(lab, rc = True):\n",
    "    print(np.unique(lab, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train the model on current ptsd and test it on current and life ptsd\n",
    "keys = ['ptsdpm_wo_NonCpGs', 'ptsdpm_wo_NonCpGsXY']\n",
    "qcd_data_ml = {k:qcd_data[k] for k in keys if k in qcd_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_data_ml.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "if p_loc == 'WS-IDRB-404B':\n",
    "    models_dir = make_directory(maindir=\"G:/PGC ML/Trained Models\",  verbose=True)\n",
    "else:\n",
    "    models_dir = make_directory(maindir=\"/work/a/ahwani/PGCML/models_dir\",  verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(models_bfs, os.path.join(models_dir, \"RF_before_fs.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_bfs.items()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will try reducing the features\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "class SelectFeatures():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function constructor\n",
    "        \n",
    "        \"\"\"\n",
    "        self.fit_lis = []\n",
    "        self.df_imp_lis = []\n",
    "        \n",
    "        \n",
    "    def CallFit(self, df, labels, top_fea):\n",
    "        \"\"\"\n",
    "        Fit model for feature selection. \n",
    "        The model will fit the data to find the predictive \n",
    "        features for response variable. \n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : List of data frames you want to select features from\n",
    "        labels : List of response variable. \n",
    "        top_fea : Number of features you want to select\n",
    "        \n",
    "        \"\"\"\n",
    "        top_selected = SelectKBest(score_func = f_classif, k = top_fea)\n",
    "        fit = top_selected.fit(df, labels)\n",
    "        self.fit_lis.append(fit)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def UnivFeatureSelection(self, df, index):\n",
    "        \"\"\"\n",
    "        This function will call the fitted models to select the features\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : List of data frames you want to select features from\n",
    "        index : Index of the model fitted on the data\n",
    "        \n",
    "        \"\"\"\n",
    "        cols = self.fit_lis[index].get_support(indices=True)\n",
    "        df_impt_uvs = df.iloc[:,cols]\n",
    "        self.df_imp_lis.append(df_impt_uvs)\n",
    "        return self.df_imp_lis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qcd_data_ml['ptsdpm'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "features = np.arange(10, 50, 10)\n",
    "#features = np.arange(10, 5000, 10)\n",
    "\n",
    "models_after_fs = defaultdict(list)\n",
    "accuracy = defaultdict(list)\n",
    "important_fea = defaultdict(list)\n",
    "train_test_imp = defaultdict(list)\n",
    "\n",
    "for i, val in enumerate(features):\n",
    "    if(val % 10 == 0):\n",
    "        print(\"# Features :\", val)\n",
    "    for key in qcd_data_ml:\n",
    "        print(key)\n",
    "        FS = SelectFeatures()\n",
    "        print(features[i])\n",
    "        \n",
    "        # remove sample identifier\n",
    "        df_ml = qcd_data_ml[key][0].loc[:,~qcd_data_ml[key][0].columns.str.contains('|'.join([\"Basename\",\n",
    "                                                                                              \"Gender\"]))]\n",
    "        FS.CallFit(df = df_ml, \n",
    "                   labels = qcd_data_ml[key][1], \n",
    "                   top_fea = features[i])\n",
    "        imp_fea = FS.UnivFeatureSelection(df = df_ml, index = 0 )\n",
    "        important_fea[key].append((val, imp_fea[0].columns))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(imp_fea[0], \n",
    "                                                            qcd_data_ml[key][1],\n",
    "                                                            test_size = 0.25,\n",
    "                                                            random_state = 0,\n",
    "                                                           stratify=qcd_data_ml[key][1])\n",
    "        train_test_imp[key].append((val, X_train, X_test, \n",
    "                                    y_train, y_test ))\n",
    "\n",
    "        clf = make_pipeline(MinMaxScaler(), \n",
    "                           LogisticRegressionCV(Cs = [0.95],\n",
    "                                                cv = 5,\n",
    "                                                solver=\"saga\",\n",
    "                                                penalty=\"elasticnet\",\n",
    "                                                l1_ratios=[0.1,0.5],\n",
    "                                                max_iter=1000,\n",
    "                                                class_weight = \"balanced\",\n",
    "                                                n_jobs = -1))\n",
    "        clf.fit(X_train, y_train)\n",
    "        prediction = clf.predict(X_test)\n",
    "        acc = clf.score(X_test, y_test)\n",
    "        clf_rep = classification_report(y_test, prediction)\n",
    "        models_after_fs[key].append((val, clf)) # store #feature and models\n",
    "        print(clf_rep)\n",
    "        print(\"Accuracy :{:.3f}\" .format(acc))\n",
    "        accuracy[key].append((val, acc)) # store #feature and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_test_imp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path on cluster\n",
    "if p_loc != 'WS-IDRB-404B':\n",
    "    fea_sets_dir = make_directory(maindir=\"/work/a/ahwani/PGCML/FeatureSets\",  verbose=True)\n",
    "    joblib.dump(important_fea, os.path.join(fea_sets_dir, \"Important Feature sets.pkl\"))\n",
    "    joblib.dump(accuracy, os.path.join(fea_sets_dir, \"Accuracy_feature_sets.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model information\n",
    "joblib.dump(clf, os.path.join(fea_sets_dir, 'readme.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models and accuracy after feature selection\n",
    "# it will be same for local and cluster\n",
    "model_name = clf.steps[1][0]\n",
    "joblib.dump(models_after_fs, os.path.join(models_dir, model_name+\"_after_fs.pkl\"))\n",
    "joblib.dump(accuracy, os.path.join(models_dir, model_name+\"_accuracy_after_fs.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
